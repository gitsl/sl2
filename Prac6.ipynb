{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6654d6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after forward propagation:\n",
      "[[0.39459663]\n",
      " [0.33849512]\n",
      " [0.32609598]\n",
      " [0.29208992]]\n",
      "Loss after forward propagation:\n",
      "0.28318958906443975\n",
      "Gradients after backward propagation:\n",
      "dW1: [[ 0.01017806 -0.01317407  0.01463282  0.00869321]\n",
      " [ 0.01072182 -0.01255784  0.00788557  0.01383816]]\n",
      "db1: [[ 0.01774608 -0.02192029  0.01406335  0.01388403]]\n",
      "dW2: [[-0.08718888]\n",
      " [-0.07236073]\n",
      " [-0.13247743]\n",
      " [-0.13562728]]\n",
      "db2: [[-0.16218059]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Mean Squared Error loss function\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    return A1, A2\n",
    "\n",
    "# Backward propagation\n",
    "def backward_propagation(X, A1, A2, y, W1, W2):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    dZ2 = A2 - y\n",
    "    dW2 = np.dot(A1.T, dZ2) / m\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "    \n",
    "    dZ1 = np.dot(dZ2, W2.T) * sigmoid_derivative(A1)\n",
    "    dW1 = np.dot(X.T, dZ1) / m\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "# Update parameters\n",
    "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example dataset (XOR)\n",
    "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "    # Initialize parameters\n",
    "    input_dim = X.shape[1]\n",
    "    hidden_dim = 4\n",
    "    output_dim = y.shape[1]\n",
    "    np.random.seed(42)\n",
    "    W1 = np.random.randn(input_dim, hidden_dim)\n",
    "    b1 = np.zeros((1, hidden_dim))\n",
    "    W2 = np.random.randn(hidden_dim, output_dim)\n",
    "    b2 = np.zeros((1, output_dim))\n",
    "\n",
    "    # Forward propagation\n",
    "    A1, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "    print(\"Output after forward propagation:\")\n",
    "    print(A2)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = mse_loss(y, A2)\n",
    "    print(\"Loss after forward propagation:\")\n",
    "    print(loss)\n",
    "    \n",
    "    # Backward propagation\n",
    "    dW1, db1, dW2, db2 = backward_propagation(X, A1, A2, y, W1, W2)\n",
    "    print(\"Gradients after backward propagation:\")\n",
    "    print(\"dW1:\", dW1)\n",
    "    print(\"db1:\", db1)\n",
    "    print(\"dW2:\", dW2)\n",
    "    print(\"db2:\", db2)\n",
    "    \n",
    "    # Update parameters\n",
    "    learning_rate = 0.1\n",
    "    W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd4298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70efc13a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b08819b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nImport NumPy: This code begins by importing the NumPy library, which is used for numerical computations.\\n\\nDefine Activation Functions:\\nsigmoid: This function implements the sigmoid activation function, which squashes the input values between 0 and 1.\\nsigmoid_derivative: This function computes the derivative of the sigmoid activation function.\\n\\nDefine Loss Function:\\nmse_loss: This function calculates the Mean Squared Error (MSE) loss between the true labels and the predicted values.\\n\\nDefine Forward Propagation:\\nforward_propagation: This function takes input data (X) and current parameters (W1, b1, W2, b2) and performs forward propagation through the neural network to generate predictions (A2).\\n\\nDefine Backward Propagation:\\nbackward_propagation: This function computes gradients of the loss function with respect to the parameters (W1, b1, W2, b2) using backpropagation.\\n\\nDefine Parameter Update:\\nupdate_parameters: This function updates the parameters (W1, b1, W2, b2) of the neural network using the computed gradients and a specified learning rate.\\n\\nExample Usage:\\nThe main section of the code initializes the example dataset (XOR) consisting of input (X) and output (y) pairs.\\nIt initializes the parameters of the neural network (W1, b1, W2, b2) with random values.\\nPerforms forward propagation to obtain predictions (A2).\\nCalculates the loss between the predictions and actual labels.\\nPerforms backward propagation to compute gradients.\\nUpdates the parameters using gradient descent.\\nPrints the output after forward propagation, loss after forward propagation, gradients after backward propagation, and updates parameters.\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Import NumPy: This code begins by importing the NumPy library, which is used for numerical computations.\n",
    "\n",
    "Define Activation Functions:\n",
    "sigmoid: This function implements the sigmoid activation function, which squashes the input values between 0 and 1.\n",
    "sigmoid_derivative: This function computes the derivative of the sigmoid activation function.\n",
    "\n",
    "Define Loss Function:\n",
    "mse_loss: This function calculates the Mean Squared Error (MSE) loss between the true labels and the predicted values.\n",
    "\n",
    "Define Forward Propagation:\n",
    "forward_propagation: This function takes input data (X) and current parameters (W1, b1, W2, b2) and performs forward propagation through the neural network to generate predictions (A2).\n",
    "\n",
    "Define Backward Propagation:\n",
    "backward_propagation: This function computes gradients of the loss function with respect to the parameters (W1, b1, W2, b2) using backpropagation.\n",
    "\n",
    "Define Parameter Update:\n",
    "update_parameters: This function updates the parameters (W1, b1, W2, b2) of the neural network using the computed gradients and a specified learning rate.\n",
    "\n",
    "Example Usage:\n",
    "The main section of the code initializes the example dataset (XOR) consisting of input (X) and output (y) pairs.\n",
    "It initializes the parameters of the neural network (W1, b1, W2, b2) with random values.\n",
    "Performs forward propagation to obtain predictions (A2).\n",
    "Calculates the loss between the predictions and actual labels.\n",
    "Performs backward propagation to compute gradients.\n",
    "Updates the parameters using gradient descent.\n",
    "Prints the output after forward propagation, loss after forward propagation, gradients after backward propagation, and updates parameters.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fa0942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
